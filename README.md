# ğŸ¬ ê°•í™”í•™ìŠµ ê¸°ë°˜ ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œ

##  ì£¼ìš” ê¸°ëŠ¥

| Îµ-Greedy, Thompson Sampling êµ¬í˜„ |
| Sequential DQN | LSTM ê¸°ë°˜ ì‹œí€€ì…œ ì¶”ì²œ ëª¨ë¸ |
| í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ | Grid Search ê¸°ë°˜ ìµœì í™” |
| ë‹¤ì¤‘ ì‹œë“œ ì‹¤í—˜ | 5ê°œ ì‹œë“œë¡œ í†µê³„ì  ì‹ ë¢°ì„± í™•ë³´ |
| TMDB API ì—°ë™ | í¬ìŠ¤í„°, ì¤„ê±°ë¦¬, í‰ì  ì •ë³´ ì œê³µ |
| ëª¨ë¸ ì €ì¥/ë¡œë“œ | í•™ìŠµëœ DQN ëª¨ë¸ ì¬ì‚¬ìš© ê°€ëŠ¥ |

---

## ğŸ›  ì„¤ì¹˜ ë°©ë²•

https://github.com/qkr-ahrdnjf/RL
í•™ìŠµëœ ëª¨ë¸ì€ 'models/dqn_model.pth'ì— í¬í•¨ë˜ì–´ ìˆìŒ.

### 3. TMDB API í‚¤ ë°œê¸‰
ì˜í™” í¬ìŠ¤í„°ì™€ ì¤„ê±°ë¦¬ë¥¼ ë³´ë ¤ë©´ TMDB API í‚¤ê°€ í•„ìš”í•˜ë‹¤.
1. https://www.themoviedb.org/ ì—ì„œ íšŒì›ê°€ì…
2. Settings â†’ API â†’ Create API Key
3. ì½”ë“œì˜ `TMDB_API_KEY` ë³€ìˆ˜ì— ì…ë ¥

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

ì‹¤í–‰ í™˜ê²½ì€ requirements.txtì— ê¸°ì¬í•¨

### Google Colabì—ì„œ ì‹¤í–‰
1. `movie_recommender_complete.ipynb` íŒŒì¼ì„ Colabì— ì—…ë¡œë“œ
2. ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ GPU ì„ íƒ
3. ì…€ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰

### í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°
ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´:

Part 3 ì½”ë“œ ë‘ ì¤„ì„ ë‹¤ìŒìœ¼ë¡œ ë°”ê¾¼ë‹¤.
final_dqn = SequentialDQNRecommender(env)
final_dqn.train(n_episodes = 500, steps_per_episode = 10)
->
final_dqn = SequentialDQNRecommender(env)
final_dqn.load_model('models/dqn_model.pth')

movie-recommender-rl/
â”‚
â”œâ”€â”€ ğŸ““ movie_recommender_complete.ipynb  
â”œâ”€â”€ ğŸ“„ movie_recommender_complete.py     
â”‚
â”œâ”€â”€ ğŸ“ models/                           # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
â”‚   â””â”€â”€ dqn_model.pth                    
â”‚
â”œâ”€â”€ ğŸ“ data/                             # ë°ì´í„° (ìë™ ë‹¤ìš´ë¡œë“œ)
â”‚   â””â”€â”€ ml-latest-small/
â”‚       â”œâ”€â”€ ratings.csv
â”‚       â””â”€â”€ movies.csv
â”‚
â”œâ”€â”€ ğŸ“ results/                          # ì‹¤í—˜ ê²°ê³¼
â”‚   â”œâ”€â”€ final_results.csv
â”‚   â”œâ”€â”€ epsilon_tuning.csv
â”‚   â””â”€â”€ dqn_tuning.csv
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt                  # í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬
â”œâ”€â”€ ğŸ“„ README.md                         
â””â”€â”€ ğŸ“„ project_report.ppt               # í”„ë¡œì íŠ¸ ë³´ê³ ì„œ


## ğŸ“– í•µì‹¬ ì½”ë“œ ì„¤ëª…

class MovieLensDataLoader:
    def __init__(self, save_dir='./data', min_user_ratings=20, min_movie_ratings=5):
        self.min_user_ratings = min_user_ratings  # ìµœì†Œ 20ê°œ í‰ì ì„ ë‚¨ê¸´ ì‚¬ìš©ìë§Œ í¬í•¨
        self.min_movie_ratings = min_movie_ratings  # ìµœì†Œ 5ê°œ í‰ì ì„ ë°›ì€ ì˜í™”ë§Œ í¬í•¨
- `min_user_ratings=20`: í‰ì ì„ 20ê°œ ì´ìƒ ë‚¨ê¸´ ì‚¬ìš©ìë§Œ ì‚¬ìš©í•œë‹¤.
- `min_movie_ratings=5`: í‰ì ì„ 5ê°œ ì´ìƒ ë°›ì€ ì˜í™”ë§Œ ì‚¬ìš©í•œë‹¤.

df['liked'] = (df['rating'] >= 4.0).astype(int)
- 5ì  ë§Œì ì˜ í‰ì ì„ ì¢‹ì•„ìš”(1) / ì‹«ì–´ìš”(0)ë¡œ ë‹¨ìˆœí™”í•œë‹¤.
- 4.0ì  ì´ìƒ â†’ `liked = 1` (ì¢‹ì•„í•¨)
- 4.0ì  ë¯¸ë§Œ â†’ `liked = 0` (ì¢‹ì•„í•˜ì§€ ì•ŠìŒ)

def temporal_train_test_split(data, test_ratio=0.2):
    for user_id, user_df in data.groupby('userId'):
        user_df = user_df.sort_values('timestamp')  
        split_idx = int(len(user_df) * 0.8)
        train = user_df.iloc[:split_idx]   
        test = user_df.iloc[split_idx:]    
- ê° ì‚¬ìš©ìë³„ë¡œ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬.
- ê³¼ê±° 80%ëŠ” í•™ìŠµì—, ë¯¸ë˜ 20%ëŠ” í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•œë‹¤.

def get_state(self, user_id):
    state = {
        'user_embed': self.user_embeddings[user_id],    
        'seq_embeds': seq_embeddings,                    
        'seq_rewards': seq_rewards                       
    }
    return state
- user_embed : SVDë¡œ ì¶”ì¶œí•œ ì‚¬ìš©ìì˜ ì·¨í–¥ ë²¡í„°. 
- seq_embeds : ìµœê·¼ì— ë³¸ ì˜í™”ì˜ ë²¡í„°ë“¤. ìµœê·¼ ê´€ì‹¬ì‚¬ë¥¼ ë°˜ì˜.
- seq_rewards : ìµœê·¼ ë³¸ ì˜í™”ë¥¼ ì¢‹ì•„í–ˆëŠ”ì§€(1) ì‹«ì–´í–ˆëŠ”ì§€(0).

def get_candidates(self, user_id, n=50):
    unwatched = self.all_movies - self.watched_movies[user_id]
    candidates = random.sample(list(unwatched), min(n, len(unwatched)))
    return candidates
- Action = ì–´ë–¤ ì˜í™”ë¥¼ ì¶”ì²œí• ì§€ ì„ íƒ
- ì´ë•Œ ê° stepì—ì„œ ì „ì²´ ì˜í™” ì¤‘ì—ì„œ 50ê°œ í›„ë³´ë§Œ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.

def step(self, action):
    movie_id = action
    if movie_id in user_test_data:
        reward = 1 if user_liked_it else 0
    else:
        reward = 1 if svd_score > 0.5 else 0
    return next_state, reward, done, next_candidates
- Reward = ì¶”ì²œí•œ ì˜í™”ë¥¼ ì‚¬ìš©ìê°€ ì¢‹ì•„í–ˆëŠ”ì§€
- ì¢‹ì•„í–ˆìœ¼ë©´ 1, ì•„ë‹ˆë©´ 0

class RandomRecommender:
    def select_action(self, state, candidates):
        return random.choice(candidates)
- ê°€ì¥ ë‹¨ìˆœí•œ ë°©ë²•ìœ¼ë¡œ ì•„ë¬´ ì˜í™”ë‚˜ ëœë¤í•˜ê²Œ ì¶”ì²œ.

class EpsilonGreedyRecommender:
    def select_action(self, state, candidates):
        if random.random() < self.epsilon:  
            return random.choice(candidates)  
        else:  
            scores = [(m, self.get_svd_score(user, m)) for m in candidates]
            return max(scores, key=lambda x: x[1])[0]
- íƒí—˜ : ëœë¤í•˜ê²Œ ì„ íƒí•´ì„œ ìƒˆë¡œìš´ ì˜í™” ë°œê²¬
- í™œìš© : ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ì§€ì‹ìœ¼ë¡œ ìµœì„ ì˜ ì„ íƒ

class ThompsonSamplingRecommender:
    def __init__(self):
        self.alpha = defaultdict(lambda: 1)  
        self.beta = defaultdict(lambda: 1)   
    
    def select_action(self, state, candidates):
        samples = []
        for movie in candidates:
            theta = np.random.beta(self.alpha[movie], self.beta[movie])
            samples.append((movie, theta))
        return max(samples, key=lambda x: x[1])[0]
    
    def update(self, movie, reward):
        if reward == 1:
            self.alpha[movie] += 1  
        else:
            self.beta[movie] += 1   
- ê° ì˜í™”ê°€ ì¢‹ì•„ìš”ë¥¼ ë°›ì„ í™•ë¥ ì„ í™•ë¥  ë¶„í¬ë¡œ ëª¨ë¸ë§.
- ë°ì´í„°ê°€ ë§ì€ ì˜í™”ëŠ” ë¶„í¬ê°€ ì¢ê³  ë°ì´í„°ê°€ ì ì€ ì˜í™”ëŠ” ë¶„í¬ê°€ ë„“ìŒ

class SequentialDQNNetwork(nn.Module):
    def __init__(self, embed_dim=20, hidden_dim=128, lstm_hidden=64):
        super().__init__()
        
        self.lstm = nn.LSTM(
            input_size=embed_dim + 1,  
            hidden_size=lstm_hidden,    
            num_layers=2,              
            batch_first=True
        )
        
        input_dim = embed_dim + lstm_hidden + embed_dim  
        self.q_network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),   
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),  
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)       
        )
- LSTM : ìµœê·¼ ë³¸ ì˜í™”ë¥¼ ë¶„ì„í•´ì„œ ì‚¬ìš©ì íŒ¨í„´ íŒŒì•…
- Q-Network : ì‚¬ìš©ì ì •ë³´ + ì‹œí€€ìŠ¤ ì •ë³´ + ì¶”ì²œí•  ì˜í™” ì •ë³´ë¥¼ í•©ì³ì„œ Qê°’ ì˜ˆì¸¡


def forward(self, user_embed, seq_embeds, seq_rewards, movie_embed):
    rewards_expanded = seq_rewards.unsqueeze(-1)  
    lstm_input = torch.cat([seq_embeds, rewards_expanded], dim=-1)  

    _, (h_n, _) = self.lstm(lstm_input)
    seq_encoding = h_n[-1]
    
    combined = torch.cat([user_embed, seq_encoding, movie_embed], dim=-1)  
    
    return self.q_network(combined)  
- ì…ë ¥: ì‚¬ìš©ì ì„ë² ë”©, ìµœê·¼ ë³¸ ì˜í™”ë“¤, ê° ì˜í™” ì¢‹ì•„ìš” ì—¬ë¶€, ì¶”ì²œí•  ì˜í™”
- ì¶œë ¥: Qê°’

class SequentialReplayBuffer:
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action_id, movie_embed, reward, next_state, next_candidates, done):
        self.buffer.append((state, action_id, movie_embed, reward, 
                           next_state, next_candidates, done))
    
    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)
- ê²½í—˜ì„ ì €ì¥í•´ë’€ë‹¤ê°€ ì—¬ëŸ¬ ë²ˆ í•™ìŠµì— ì¬ì‚¬ìš©.
- ë°ì´í„° íš¨ìœ¨ì„±ì´ ë†’ì•„ì§€ê³ , í•™ìŠµì´ ì•ˆì •ì .

def _compute_max_next_q(self, next_user_embed, next_seq_embeds, next_seq_rewards, next_candidates):
    if len(next_candidates) == 0:
        return 0.0
    
    if len(next_candidates) > self.max_next_candidates:
        sampled = random.sample(next_candidates, self.max_next_candidates)
    else:
        sampled = next_candidates
    
    max_q = float('-inf')
    for movie_id in sampled:
        movie_embed = self.env.get_movie_embedding(movie_id)
        movie_embed = torch.FloatTensor(movie_embed).unsqueeze(0).to(device)
        
        q = self.target_net(next_user_embed, next_seq_embeds, next_seq_rewards, movie_embed)
        max_q = max(max_q, q.item())
    
    return max_q
- Q-learning : Target = reward + Î³ Ã— max Q(s', a') = ì§€ê¸ˆ ë°›ì€ ë³´ìƒ + ë¯¸ë˜ì— ë°›ì„ ìˆ˜ ìˆëŠ” ìµœëŒ€ ë³´ìƒ

def train_step(self):
    batch = self.buffer.sample(self.batch_size)
    
    current_q = self.policy_net(
        batch['user_embeds'], batch['seq_embeds'], 
        batch['seq_rewards'], batch['movie_embeds']
    ).squeeze()
    
    with torch.no_grad():
        target_q_values = []
        for i in range(self.batch_size):
            reward = batch['rewards'][i].item()
            done = batch['dones'][i].item()
            
            if done:
                target_q = reward  
            else:
                max_next_q = self._compute_max_next_q(...)
                target_q = reward + self.gamma * max_next_q
            
            target_q_values.append(target_q)
    
    loss = F.smooth_l1_loss(current_q, torch.FloatTensor(target_q_values))
    
    self.optimizer.zero_grad()
    loss.backward()
    self.optimizer.step()
- í˜„ì¬ Qê°’ì´ Target Qê°’ì— ê°€ê¹Œì›Œì§€ë„ë¡ í•™ìŠµ

def ndcg_at_k(self, user_id, recs, k):
    actual = self.user_ground_truth[user_id]
    
    dcg = sum(1.0 / np.log2(i + 2) for i, m in enumerate(recs[:k]) if m in actual)
    
    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(actual), k)))
    
    return dcg / idcg if idcg > 0 else 0.0
- ìƒìœ„ì— ìˆì„ìˆ˜ë¡ ì ìˆ˜ë¥¼ ë” ì–»ìŒ.

def save_model(self, path):
    torch.save({
        'policy_net': self.policy_net.state_dict(),
        'target_net': self.target_net.state_dict(),
        'optimizer': self.optimizer.state_dict(),
        'epsilon': self.epsilon
    }, path)
    print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")
def load_model(self, path):
    checkpoint = torch.load(path)
    self.policy_net.load_state_dict(checkpoint['policy_net'])
    self.target_net.load_state_dict(checkpoint['target_net'])
    self.optimizer.load_state_dict(checkpoint['optimizer'])
    self.epsilon = checkpoint['epsilon']
    print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path}")

## í•™ìŠµëœ ëª¨ë¸

`models/` í´ë”ì— í•™ìŠµëœ DQN ëª¨ë¸ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
models/
â””â”€â”€ dqn_model.pth
```
### ëª¨ë¸ ì‚¬ìš© ë°©ë²•
# 1. í™˜ê²½ ë° ì¶”ì²œê¸° ì´ˆê¸°í™”
env = RecommendationEnv(train_data, test_data, ...)
recommender = SequentialDQNRecommender(env)

# 2. ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ
recommender.load_model('models/dqn_model.pth')

# 3. ì¶”ì²œ ë°›ê¸° (í•™ìŠµ ì—†ì´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥)
recommendations = recommender.get_recommendations(user_id=42, k=10)
for rec in recommendations:
    print(f"ì˜í™” ID: {rec['movieId']}, Qê°’: {rec['score']:.4f}")
